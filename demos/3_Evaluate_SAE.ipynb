{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df783e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853798d6",
   "metadata": {},
   "source": [
    "# Evaluate vision SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33387114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 23:27:14 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-06-01 23:27:14 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/resolve/main/weights.pt HTTP/1.1\" 302 0\n",
      "2025-06-01 23:27:14 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /home/mila/s/sonia.joseph/.cache/huggingface/hub/models--Prisma-Multimodal--sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/snapshots/b46d6e9d6c114a53364c5c172ea5023e0e52d4e2/weights.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 23:27:15 WARNING:root: Deprecated field 'total_training_images' found in config. It will be ignored.\n",
      "2025-06-01 23:27:15 WARNING:root: Deprecated field 'total_training_tokens' found in config. It will be ignored.\n",
      "2025-06-01 23:27:15 WARNING:root: Deprecated field 'd_sae' found in config. It will be ignored.\n",
      "2025-06-01 23:27:15 INFO:root: n_tokens_per_buffer (millions): 0.032\n",
      "2025-06-01 23:27:15 INFO:root: Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "2025-06-01 23:27:15 INFO:root: Total training steps: 158691\n",
      "2025-06-01 23:27:15 INFO:root: Total training images: 13000000\n",
      "2025-06-01 23:27:15 INFO:root: Total wandb updates: 1586\n",
      "2025-06-01 23:27:15 INFO:root: Expansion factor: 64\n",
      "2025-06-01 23:27:15 INFO:root: n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "2025-06-01 23:27:15 INFO:root: n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "2025-06-01 23:27:15 INFO:root: We will reset the sparsity calculation 158 times.\n",
      "2025-06-01 23:27:15 INFO:root: Number tokens in sparsity calculation window: 4.10e+06\n",
      "2025-06-01 23:27:15 INFO:root: Using Ghost Grads.\n",
      "2025-06-01 23:27:15 INFO:root: Gradient clipping with max_norm=1.0\n",
      "2025-06-01 23:27:15 INFO:root: Using SAE initialization method: encoder_transpose_decoder\n",
      "2025-06-01 23:27:15 INFO:root: get_activation_fn received: activation_fn=relu, kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardSparseAutoencoder(\n",
       "  (hook_sae_in): HookPoint()\n",
       "  (hook_hidden_pre): HookPoint()\n",
       "  (hook_hidden_post): HookPoint()\n",
       "  (hook_sae_out): HookPoint()\n",
       "  (activation_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load SAE\n",
    "\n",
    "# Load an SAE\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_sae(repo_id, file_name, config_name):\n",
    "    # Step 1: Download SAE weights and SAE config from Hugginface\n",
    "    sae_path = hf_hub_download(repo_id, file_name) # Download SAE weights\n",
    "    hf_hub_download(repo_id, config_name) # Download SAE config\n",
    "\n",
    "    # Step 2: Now load the pretrained SAE weights from where you just downloaded them\n",
    "    print(f\"Loading SAE from {sae_path}...\")\n",
    "    sae = SparseAutoencoder.load_from_pretrained(sae_path) # This now automatically gets the config.json file in that folder and converts it into a VisionSAERunnerConfig object\n",
    "    return sae\n",
    "\n",
    "# Change the repo_id to the Huggingface repo of your chosen SAE. See /docs for a list of SAEs.\n",
    "repo_id = \"Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05\" \n",
    "file_name = \"weights.pt\" # Usually weights.pt but may have slight naming variation. See the chosen HF repo for the exact file name\n",
    "config_name = \"config.json\"\n",
    "\n",
    "sae = load_sae(repo_id, file_name, config_name)\n",
    "sae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4eebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 23:27:15 INFO:root: Model 'open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K' is supported and passes tests.\n",
      "2025-06-01 23:27:15 INFO:root: model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "2025-06-01 23:27:15 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/resolve/main/open_clip_config.json HTTP/1.1\" 200 0\n",
      "2025-06-01 23:27:15 INFO:root: model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "2025-06-01 23:27:15 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/resolve/main/open_clip_pytorch_model.bin HTTP/1.1\" 302 0\n",
      "2025-06-01 23:27:16 INFO:root: visual projection shape: torch.Size([768, 512])\n",
      "2025-06-01 23:27:16 INFO:root: Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedViT(\n",
       "  (embed): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  )\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbedding()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (hook_full_embed): HookPoint()\n",
       "  (ln_pre): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_pre): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_final): HookPoint()\n",
       "  (head): Head()\n",
       "  (hook_post_head_pre_normalize): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from vit_prisma.models.model_loader import load_hooked_model\n",
    "\n",
    "\n",
    "model_name = sae.cfg.model_name\n",
    "model = load_hooked_model(model_name)\n",
    "model.to(DEVICE) # Move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9abd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 23:28:04 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/resolve/main/open_clip_pytorch_model.bin HTTP/1.1\" 302 0\n",
      "2025-06-01 23:28:04 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/resolve/main/open_clip_config.json HTTP/1.1\" 200 0\n",
      "2025-06-01 23:28:04 INFO:root: Loaded hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K model config.\n",
      "2025-06-01 23:28:05 INFO:root: Loading pretrained hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K weights (/home/mila/s/sonia.joseph/.cache/huggingface/hub/models--laion--CLIP-ViT-B-32-DataComp.XL-s13B-b90K/snapshots/f0e2ffa09cbadab3db6a261ec1ec56407ce42912/open_clip_pytorch_model.bin).\n",
      "2025-06-01 23:28:06 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/resolve/main/open_clip_config.json HTTP/1.1\" 200 0\n",
      "2025-06-01 23:28:13 INFO:vit_prisma.sae.evals.model_eval: Starting SparseCoder evaluation...\n",
      "Evaluating: 100%|██████████| 250/250 [00:21<00:00, 11.87it/s, L0=2992.25, Cosine Sim=0.991065]\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Finished running through validation dataset...\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Computing metrics...\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average L0 (features activated): 777.640015\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average L0 (features activated) per CLS token: 893.646973\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average L0 (features activated) per image: 3047.367000\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average Cosine Similarity: 0.9915\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average Loss: 6.722949\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average Reconstruction Loss: 6.722559\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average Zero Ablation Loss: 6.740437\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: Average CE Score: nan\n",
      "2025-06-01 23:28:34 INFO:vit_prisma.sae.evals.model_eval: % CE recovered: 102.227793\n"
     ]
    }
   ],
   "source": [
    "# Evaluate vision SAE\n",
    "from vit_prisma.sae import SparsecoderEval\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "\n",
    "import torchvision\n",
    "\n",
    "imagenet_validation_path = '/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val'\n",
    "\n",
    "data_transforms = get_clip_val_transforms()\n",
    "eval_dataset = torchvision.datasets.ImageFolder(imagenet_validation_path, transform=data_transforms)\n",
    "\n",
    "eval_runner = SparsecoderEval(sae, model, eval_dataset, max_images=1000)\n",
    "\n",
    "metrics = eval_runner.run_eval(is_clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e474c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
